{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ íŒŒì´ì¬ ë¨¸ì‹ ëŸ¬ë‹ ì™„ë²½ ê°€ì´ë“œ í˜¼ê³µ\n",
    "\n",
    "### 2019.07.10 ~ êµì¬ 8ì¥\n",
    "\n",
    "### 08. í…ìŠ¤íŠ¸ ë¶„ì„\n",
    "\n",
    "## 01. í…ìŠ¤íŠ¸ ë¶„ì„ ì´í•´ \n",
    "í…ìŠ¤íŠ¸ë¥¼ í”¼ì²˜ ë²¡í„°í™” í•´ì£¼ì–´ì•¼ í•˜ëŠ”ë° 1) BOW 2) Word2Vec ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì—ì„œëŠ” BOWë§Œ í•™ìŠµí•  ê²ƒ.\n",
    "\n",
    " í…ìŠ¤íŠ¸ ë¶„ì„ ìˆ˜í–‰ í”„ë¡œì„¸ìŠ¤ \n",
    "1. í…ìŠ¤íŠ¸ ì‚¬ì „ ì¤€ë¹„ì‘ì—…(í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬) : í´ë Œì§•, ëŒ€/ì†Œë¬¸ì ë³€ê²½, íŠ¹ìˆ˜ë¬¸ì ì‚­ì œ ë“±ì˜ í´ë Œì§• ì‘ì—…, ë‹¨ì–´ ë“±ì˜ í† í°í™” ì‘ì—…, ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´(stopword) ì œê±° ì‘ì—…, ì–´ê·¼ ì¶”ì¶œ(Stemming/Lemmatization) ë“±ì˜ í…ìŠ¤íŠ¸ ì •ê·œí™” ì‘ì—… ìˆ˜í–‰ \n",
    "2. í”¼ì²˜ ë²¡í„°í™”/ì¶”ì¶œ: ê°€ê³µëœ í…ìŠ¤íŠ¸ì—ì„œ í”¼ì²˜ë¥¼ ì¶”ì¶œí•˜ê³  ë²¡í„°ê°’ í• ë‹¹. BOWë‚˜ Word2Vecì´ ëŒ€í‘œì ìœ¼ë¡œ ì‚¬ìš©ë˜ë©°, BOWëŠ” ëŒ€í‘œì ìœ¼ë¡œ Count ê¸°ë°˜ê³¼ TF_IDF ê¸°ë°˜ ë²¡í„°í™” ìˆìŒ. \n",
    "3. ML ëª¨ë¸ ìˆ˜ë¦½ ë° í•™ìŠµ/ì˜ˆì¸¡/í‰ê°€: í”¼ì²˜ ë²¡í„°í™”ëœ ë°ì´í„° ì„¸íŠ¸ì— ML ëª¨ë¸ ì ìš©í•´ í•™ìŠµ/ì˜ˆì¸¡ ë° í‰ê°€ë¥¼ ìˆ˜í–‰\n",
    "\n",
    "### í…ìŠ¤íŠ¸ ë¶„ì„ íŒ¨í‚¤ì§€ \n",
    "1. NLTK \n",
    "2. Gensim: Word2Vec êµ¬í˜„. í† í”½ ëª¨ë¸ë§ ë¶„ì•¼ì—ì„œ ë‘ê°ì„ ë‚˜íƒ€ë‚´ëŠ” íŒ¨í‚¤ì§€\n",
    "3. SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. í…ìŠ¤íŠ¸ ì‚¬ì „ ì¤€ë¹„ ì‘ì—… (í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬) - í…ìŠ¤íŠ¸ ì •ê·œí™” \n",
    "\n",
    "### í´ë Œì§• \n",
    "ë¶ˆí•„ìš”í•œ ë¬¸ì, ê¸°í˜¸ ë“±ì„ ì‚¬ì „ì— ì œê±°í•˜ëŠ” ì‘ì—…. \n",
    "\n",
    "### í…ìŠ¤íŠ¸ í† í°í™” \n",
    "ë¬¸ì„œì—ì„œ ë¬¸ì¥ì„ ë¶„ë¦¬í•˜ëŠ” í† í°í™”ì™€ ë¬¸ì¥ì—ì„œ ë‹¨ì–´ë¥¼ í† í°ìœ¼ë¡œ ë¶„ë¦¬í•˜ëŠ” ë‹¨ì–´ í† í°í™”\n",
    "\n",
    "1. ë¬¸ì¥ í† í°í™”: \n",
    "ë§ˆì¹¨í‘œë‚˜ ê°œí–‰ë¬¸ì(\\n) ë“±ì˜ ë¬¸ì¥ì˜ ë§ˆì§€ë§‰ì„ ëœ»í•˜ëŠ” ê¸°í˜¸ì— ë”°ë¼ ë¶„ë¦¬í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì . ì •ê·œí‘œí˜„ì‹ì— ë”°ë¥¸ ë¬¸ì¥ í† í°í™”ë„ ê°€ëŠ¥.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì¥ í† í°í™”\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all arouns us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/harampark/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all arouns us, here even in this room. \\\n",
    "              You can see it out your window or on your television. \\\n",
    "              You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ë‹¨ì–´ í† í°í™”: \n",
    "ë¬¸ì¥ì„ ë‹¨ì–´ë¡œ í† í°í™”í•˜ëŠ” ê²ƒ. BOWì™€ ê°™ì´ ë‹¨ì–´ì˜ ìˆœì„œê°€ ì¤‘ìš”í•˜ì§€ ì•Šì€ ê²½ìš° ë¬¸ì¥ í† í°í™”ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë‹¨ì–´ í† í°í™”ë§Œ ì‚¬ìš©í•´ë„ ì¶©ë¶„í•¨. ê°ë¬¸ì¥ì´ ê°€ì§€ëŠ” ì‹œë§¨í‹±ì ì¸ ì˜ë¯¸ê°€ ì¤‘ìš”í•œ ìš”ì†Œë¡œ ì‚¬ìš©ë  ë•Œ ì‚¬ìš©í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize \n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¬¸ì¥ í† í°í™”ì™€ ë‹¨ì–´ í† í°í™”ë¥¼ ì¡°í•©í•´ ë¬¸ì„œì— ëŒ€í•´ ëª¨ë“  ë‹¨ì–´ë¥¼ í† í°í™” í•´ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'arouns', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize \n",
    "\n",
    "# ì—¬ëŸ¬ ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ ëœ ì…ë ¥ ë°ì´í„°ë¥¼ ë¬¸ì¥ë³„ë¡œ ë‹¨ì–´ í† í°í™”í•˜ê²Œ ë§Œë“œëŠ” í•¨ìˆ˜ ìƒì„±\n",
    "def tokenize_text(text): \n",
    "  sentences = sent_tokenize(text)\n",
    "  word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "  return word_tokens\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë ‡ê²Œ ë¬¸ì¥ì„ ë‹¨ì–´ë³„ë¡œ í•˜ë‚˜ì”© í† í°í™”í•  ê²½ìš° ë¬¸ë§¥ì ì¸ ì˜ë¯¸ëŠ” ë¬´ì‹œë  ìˆ˜ë°–ì— ì—†ìŒ. ì´ë¥¼ ì¡°ê¸ˆì´ë¼ë„ í•´ê²°í•´ë³´ê³ ì í•œ ê²ƒì´ **n-gram**ì´ë‹¤. ì—°ì†ëœ nê°œì˜ ë‹¨ì–´ë¥¼ í•˜ë‚˜ì˜ í† í°í™” ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•´ë‚´ëŠ” ê²ƒ. nê°œ ë‹¨ì–´ í¬ê¸° ìœˆë„ìš°ë¥¼ ë§Œë“¤ì–´ ë¬¸ì¥ì˜ ì²˜ìŒë¶€í„° ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì›€ì§ì´ë©´ì„œ í† í°í™”ë¥¼ ìˆ˜í–‰í•¨. \n",
    "\n",
    "### ìŠ¤í†± ì›Œë“œ ì œê±° \n",
    "ë¶„ì„ì— í° ì˜ë¯¸ê°€ ì—†ëŠ” ë‹¨ì–´ë¥¼ ì§€ì¹­. ì–¸ì–´ë³„ë¡œ ì´ëŸ¬í•œ ìŠ¤í†± ì›Œë“œê°€ ëª©ë¡í™”ë¼ ìˆìŒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/harampark/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜ì–´ wtop words ê°œìˆ˜: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "print('ì˜ì–´ wtop words ê°œìˆ˜:', len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'arouns', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "# stopwords ì œê±°í•´ ì˜ë¯¸ìˆëŠ” ê²ƒë§Œ ë‚¨ê²¨ë³´ì \n",
    "import nltk \n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "for sentence in word_tokens: \n",
    "  filtered_words=[]\n",
    "  for word in sentence: \n",
    "    word = word.lower()\n",
    "    if word not in stopwords: \n",
    "      filtered_words.append(word)\n",
    "  all_tokens.append(filtered_words)\n",
    "\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemmingê³¼ Lemmatization \n",
    "\n",
    "ë¬¸ë²•ì  ë˜ëŠ” ì˜ë¯¸ì ìœ¼ë¡œ ë³€í™”í•˜ëŠ” ë‹¨ì–´ì˜ ì›í˜•ì„ ì°¾ëŠ” ê²ƒ. Lemmatizationì´ Stemmingë³´ë‹¤ ì •êµí•˜ë©° ì˜ë¯¸ë¡ ì  ê¸°ë°˜ì—ì„œ ë‹¨ì–´ì˜ ì›í˜•ì„ ì°¾ëŠ”ë‹¤. Lemmatizationì€ í’ˆì‚¬ì™€ ê°™ì€ ë¬¸ë²•ì ì¸ ìš”ì†Œì™€ ë” ì˜ë¯¸ì ì¸ ë¶€ë¶„ì„ ê°ì•ˆí•´ ì •í™•í•œ ì² ìë¡œ ëœ ì–´ê·¼ ë‹¨ì–´ë¥¼ ì°¾ì•„ì¤€ë‹¤. ê·¸ë˜ì„œ ì‹œê°„ì´ ë” ì˜¤ë˜ ê±¸ë¦¼. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "# Stemming \n",
    "from nltk.stem import LancasterStemmer \n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
    "# ìµœìƒê¸‰ì€ ì œëŒ€ë¡œ ì¸ì‹ ëª»í•¨\n",
    "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/harampark/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization - ì •í™•í•œ ì›í˜• ë‹¨ì–´ ì¶”ì¶œìœ„í•´ ë‹¨ì–´ì˜ 'í’ˆì‚¬'ë¥¼ ì…ë ¥\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk \n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing','v'), lemma.lemmatize('amuses','v'), lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'), lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier','a'), lemma.lemmatize('fanciest','a'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Bag of Words - BOW \n",
    "\n",
    "ë¬¸ì„œê°€ ê°€ì§€ëŠ” ëª¨ë“  ë‹¨ì–´(Words)ë¥¼ ë¬¸ë§¥ì´ë‚˜ ìˆœì„œë¥¼ ë¬´ì‹œí•˜ê³  ì¼ê´„ì ìœ¼ë¡œ ë‹¨ì–´ì— ëŒ€í•´ ë¹ˆë„ ê°’ì„ ë¶€ì—¬í•´ í”¼ì²˜ ê°’ì„ ì¶”ì¶œí•˜ëŠ” ëª¨ë¸. ì¥ì ì€ ì‰½ê³  ë¹ ë¥¸ êµ¬ì¶•. ê·¸ëŸ¬ë‚˜ ì—¬ëŸ¬ê°€ì§€ ì œì•½ì´ ì¡´ì¬. \n",
    "1. ë¬¸ë§¥ ì˜ë¯¸(Semantic Context) ë°˜ì˜ ë¶€ì¡±: ë‹¨ì–´ì˜ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ì•Šì•„ ë¬¸ì¥ ë‚´ì—ì„œ ë¬¸ë§¥ì ì¸ ì˜ë¯¸ê°€ ë¬´ì‹œë¨. \n",
    "2. í¬ì†Œ í–‰ë ¬ ë¬¸ì œ(í¬ì†Œì„±, í¬ì†Œ í–‰ë ¬): BOWë¡œ í”¼ì²˜ ë°±í„°í™”ë¥¼ ìˆ˜í–‰í•˜ë©´ í¬ì†Œ í–‰ë ¬ í˜•íƒœì˜ ë°ì´í„° ì„¸íŠ¸ê°€ ë§Œë“¤ì–´ì§€ê¸° ì‰¬ì›€. í¬ì†Œí–‰ë ¬(ëŒ€ê·œëª¨ì˜ ì¹¼ëŸ¼ìœ¼ë¡œ êµ¬ì„±ëœ í–‰ë ¬ì—ì„œ ëŒ€ë¶€ë¶„ì˜ ê°’ì´ 0ìœ¼ë¡œ ì±„ì›Œì§€ëŠ” í–‰ë ¬ - ëª¨ë“  ë¬¸ì„œì— ë˜‘ê°™ì€ ë‹¨ì–´ê°€ ë“¤ì–´ê°€ì§€ ì•Šì•„ 0ìœ¼ë¡œ ì±„ì›Œì§ˆ í™•ë¥ ì´ ë” ë†’ìŒ) ML ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜í–‰ì‹œê°„ê³¼ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë–¨ì–´ëœ¨ë¦¬ê¸° ë•Œë¬¸ì— í¬ì†Œí–‰ë ¬ì„ ìœ„í•œ íŠ¹ë³„í•œ ê¸°ë²•ì´ ë§ˆë ¨ë˜ì–´ ìˆìŒ.\n",
    "\n",
    "### BOW í”¼ì²˜ ë²¡í„°í™” \n",
    "í…ìŠ¤íŠ¸ë¥¼ íŠ¹ì • ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” ìˆ«ìí˜• ê°’ì¸ ë²¡í„°ê°’ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•˜ëŠ”ë°, ì´ëŸ¬í•œ ë³€í™˜ì„ í”¼ì²˜ ë²¡í„°í™”ë¼ê³  í•¨. ëª¨ë“  ë¬¸ì„œì—ì„œ ëª¨ë“  ë‹¨ì–´ë¥¼ ì¹¼ëŸ¼ í˜•íƒœë¡œ ë‚˜ì—´í•˜ê³  ê° ë¬¸ì„œì—ì„œ í•´ë‹¹ ë‹¨ì–´ì˜ íšŸìˆ˜ë‚˜ ì •ê·œí™”ëœ ë¹ˆë„ë¥¼ ê°’ìœ¼ë¡œ ë¶€ì—¬í•˜ëŠ” ë°ì´í„° ì„¸íŠ¸ ëª¨ë¸ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒ. ê²°ê³¼ì ìœ¼ë¡œ M(ë¬¸ì„œê°œìˆ˜)*N(ë‹¨ì–´ featureë“¤ nê°œ)ì˜ ë‹¨ì–´ í”¼ì²˜ë¡œ ì´ë¤„ì§„ í–‰ë ¬ì„ êµ¬ì„±í•¨.\n",
    "\n",
    "BOWì˜ í”¼ì²˜ ë²¡í„°í™”ì˜ 2ê°€ì§€ ë°©ì‹\n",
    "- ì¹´ìš´íŠ¸ ê¸°ë°˜ì˜ ë²¡í„°í™”: ì¹´ìš´íŠ¸ê°’ì´ ë†’ì„ìˆ˜ë¡ ì¤‘ìš”í•œ ë‹¨ì–´ë¡œ ì¸ì‹\n",
    "- TF-IDF ê¸°ë°˜ì˜ ë²¡í„°í™”: ê°œë³„ ë¬¸ì„œì—ì„œ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ì— ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ë˜, ëª¨ë“  ë¬¸ì„œì—ì„œ ì „ë°˜ì ìœ¼ë¡œ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ì— ëŒ€í•´ì„œëŠ” í˜ë„í‹°ë¥¼ ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ê°’ì„ ë¶€ì—¬. ë¬¸ì„œë§ˆë‹¤ í…ìŠ¤íŠ¸ê°€ ê¸¸ê³  ë¬¸ì„œì˜ ê°œìˆ˜ê°€ ë§ì€ ê²½ìš° ì¹´ìš´íŠ¸ ë°©ì‹ë³´ë‹¤ëŠ” TF-IDF ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì¢‹ì€ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë³´ì¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì‚¬ì´í‚·ëŸ°ì˜ Count ë° TF-IDF ë²¡í„°í™” êµ¬í˜„: CountVectorizer, TfidfVectorizer \n",
    "\n",
    "#### ì‚¬ì´í‚·ëŸ°ì˜ CountVectorizer\n",
    "ì¹´ìš´íŠ¸ ê¸°ë°˜ì˜ ë²¡í„°í™”ë¥¼ êµ¬í˜„í•œ í´ë˜ìŠ¤. í”¼ì²˜ ë²¡í„°í™”ë§Œ ìˆ˜í–‰í•˜ì§€ ì•Šìœ¼ë©° ì†Œë¬¸ì ì¼ê´„ ë³€í™˜, í† í°í™”, ìŠ¤í†± ì›Œë“œ í•„í„°ë§ ë“±ì˜ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë„ í•¨ê»˜ ìˆ˜í–‰í•¨. fit()ê³¼ transform()ì„ í†µí•´ í”¼ì²˜ ë²¡í„°í™”ëœ ê°ì²´ë¥¼ ë°˜í™˜í•¨.\n",
    "\n",
    "#### ì‚¬ì´í‚·ëŸ°ì˜ TfidfVectorizer\n",
    "TF-IDF ë²¡í„°í™” í´ë˜ìŠ¤ \n",
    "\n",
    "- max_df: ì „ì²´ ë¬¸ì„œì— ê±¸ì³ì„œ ë„ˆë¬´ ë†’ì€ ë¹ˆë„ìˆ˜ë¥¼ ê°€ì§€ëŠ” ë‹¨ì–´ í”¼ì²˜ë¥¼ ì œì™¸í•˜ê¸°ìœ„í•œ íŒŒë¼ë¯¸í„° \n",
    "- min_df: ì „ì²´ ë¬¸ì„œì— ê±¸ì³ì„œ ë„ˆë¬´ ë‚®ì€ ë¹ˆë„ìˆ˜ë¥¼ ê°€ì§€ëŠ” ë‹¨ì–´ í”¼ì²˜ë¥¼ ì œì™¸í•˜ê¸°ìœ„í•œ íŒŒë¼ë¯¸í„° \n",
    "- max_features: ì¶”ì¶œí•˜ëŠ” í”¼ì²˜ì˜ ê°œìˆ˜ë¥¼ ì œí•œí•˜ë©° ì •ìˆ˜ë¡œ ê°’ì„ ì§€ì •í•¨.\n",
    "- stop_words\n",
    "- n_gram_range: ë‹¨ì–´ ìˆœì„œë¥¼ ë³´ê°•í•˜ê¸°ìœ„í•œ n_gram ë²”ìœ„ ì„¤ì •, íŠœí”Œ í˜•íƒœë¡œ (ë²”ìœ„ ìµœì†Ÿê°’, ë²”ìœ„ ìµœëŒ“ê°’)ì„ ì§€ì •\n",
    "- analyzer: í”¼ì²˜ ì¶”ì¶œì„ ìˆ˜í–‰í•œ ë‹¨ìœ„ ì§€ì •. ë””í´íŠ¸ëŠ” 'word' \n",
    "- token_pattern: í† í°í™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì •ê·œ í‘œí˜„ì‹ íŒ¨í„´ ì§€ì • \n",
    "- tokenizer: í† í°í™”ë¥¼ ë³€ë„ì˜ ì»¤ìŠ¤í…€ í•¨ìˆ˜ë¡œ ì´ìš©ì‹œ ì ìš© \n",
    "\n",
    "#### ìˆœì„œ \n",
    "1. ì‚¬ì „ ë°ì´í„° ê°€ê³µ: ëª¨ë“  ë¬¸ìë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ëŠ” ë“±ì˜ ì‚¬ì „ ì‘ì—… ìˆ˜í–‰\n",
    "2. í† í°í™”: ë‹¨ì–´ ê¸°ì¤€ (default), n_gram_rangeë¥¼ ë°˜ì˜í•´ í† í°í™” ìˆ˜í–‰\n",
    "3. í…ìŠ¤íŠ¸ ì •ê·œí™”: stopwords í•„í„°ë§ë§Œ ìˆ˜í–‰. Stemmer, LemmatizeëŠ” CountVectorizer ìì²´ì—ì„œëŠ” ì§€ì›ë˜ì§€ ì•ŠìŒ. ì´ë¥¼ ìœ„í•œ í•¨ìˆ˜ë¥¼ ë§Œë“¤ê±°ë‚˜ ì™¸ë¶€ íŒ¨í‚¤ì§€ë¡œ ë¯¸ë¦¬ Text Normalization ìˆ˜í–‰ í•„ìš” \n",
    "4. í”¼ì²˜ ë²¡í„°í™”: ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°ë¥¼ ë°˜ì˜í•´ Tokenëœ ë‹¨ì–´ë“¤ì„ feature extraction í›„ vectorization ì ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW ë²¡í„°í™”ë¥¼ ìœ„í•œ í¬ì†Œ í–‰ë ¬\n",
    "ì‚¬ì´í‚·ëŸ°ì˜ CountVectorizer/TfidfVectorizerë¥¼ ì´ìš©í•´ í…ìŠ¤íŠ¸ë¥¼ í”¼ì²˜ ë‹¨ìœ„ë¡œ ë²¡í„°í™”í•´ ë³€í™˜í•˜ê³  CSR í˜•íƒœì˜ í¬ì†Œ í–‰ë ¬ì„ ë°˜í™˜í•¨. ì´ì²˜ëŸ¼ ëŒ€ê·œëª¨ í–‰ë ¬ì˜ ëŒ€ë¶€ë¶„ì˜ ê°’ì„ 0ì´ ì°¨ì§€í•˜ëŠ” í–‰ë ¬ì„ ê°€ë¦¬ì¼œ í¬ì†Œí–‰ë ¬ì´ë¼ê³  í•˜ê³ , BOW í˜•íƒœë¥¼ ê°€ì§„ ì–¸ì–´ ëª¨ë¸ì˜ í”¼ì²˜ ë²¡í„°í™”ëŠ” ëŒ€ë¶€ë¶„ í¬ì†Œí–‰ë ¬ì„. ë„ˆë¬´ ë§ì€ ë¶ˆí•„ìš”í•œ 0ê°’ì´ ë©”ëª¨ë¦¬ ê³µê°„ì— í• ë‹¹ë˜ê¸° ë•Œë¬¸ì— ì—°ì‚° ì‹œì—ë„ ë°ì´í„° ì•¡ì„¸ìŠ¤ë¥¼ ìœ„í•œ ì‹œê°„ì´ ë§ì´ ì†Œìš”ë¨. ê·¸ë˜ì„œ ì ì€ ë©”ëª¨ë¦¬ ê³µê°„ì„ ì°¨ì§€í•  ìˆ˜ ìˆë„ë¡ ë³€í™˜í•´ì•¼ í•˜ëŠ”ë° ëŒ€í‘œì ì¸ ë°©ë²•ì´ **COO í˜•ì‹ê³¼ CSR í˜•ì‹**ì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ í° í¬ì†Œ í–‰ë ¬ì„ ì €ì¥í•˜ê³  ê³„ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ëŠ¥ë ¥ì´ CSR í˜•ì‹ì´ ë” ë›°ì–´ë‚˜ê¸° ë•Œë¬¸ì— CSRì„ ë§ì´ ì‚¬ìš©í•¨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### í¬ì†Œ í–‰ë ¬ - COO í˜•ì‹\n",
    "COO í˜•ì‹ì€ 0ì´ ì•„ë‹Œ ë°ì´í„°ë§Œ ë³„ë„ì˜ ë°ì´í„° ë°°ì—´(array)ì— ì €ì¥í•˜ê³  ê·¸ ë°ì´í„°ê°€ ê°€ë¦¬í‚¤ëŠ” í–‰ê³¼ ì—´ì˜ ìœ„ì¹˜ë¥¼ ë³„ë„ì˜ ë°°ì—´ë¡œ ì €ì¥í•˜ëŠ” ë°©ì‹. í¬ì†Œí–‰ë ¬ ë³€í™˜ì„ ìœ„í•´ íŒŒì´ì¬ ì„¸ê³„ì—ì„œëŠ” ì£¼ë¡œ ì‚¬ì´íŒŒì´(scipy) ì´ìš©í•¨. ì‚¬ì´íŒŒì´ì˜ sparse íŒ¨í‚¤ì§€ëŠ” í¬ì†Œí–‰ë ¬ ë³€í™˜ì„ ìœ„í•œ ë‹¤ì–‘í•œ ëª¨ë“ˆì„ ì œê³µí•¨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "dense = np.array([[3,0,1], [0,2,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse \n",
    "\n",
    "# 0ì´ ì•„ë‹Œ ë°ì´í„° ì¶”ì¶œ\n",
    "data = np.array([3,1,2])\n",
    "\n",
    "# í–‰ ìœ„ì¹˜ì™€ ì—´ ìœ„ì¹˜ë¥¼ ê°ê° ë°°ì—´ë¡œ ìƒì„±\n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "\n",
    "# sparse íŒ¨í‚¤ì§€ì˜ coo_matrixë¥¼ ì´ìš©í•´ COO í˜•ì‹ìœ¼ë¡œ í¬ì†Œ í–‰ë ¬ ìƒì„±\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë‹¤ì‹œ ì›ë˜ì˜ ë°ì´í„° í–‰ë ¬ë¡œ ì¶”ì¶œë¨ \n",
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
