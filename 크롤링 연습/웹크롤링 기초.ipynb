{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 웹 크롤링 기초 \n",
    "\n",
    "패턴으로 실습하며 익히기: 크롤링 코드 패턴으로 익히기1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 라이브러리 임포트\n",
    "\n",
    "requests : 웹페이지 가져오기 라이브러리\n",
    "\n",
    "bs4 : 웹페이지 분석(크롤링) 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 웹페이지 가져오기\n",
    "\n",
    "(컴퓨터 속 부품) 네트워크는 내 컴퓨터와 다른 컴퓨터를 연결해 정보를 주고 받는 것. \n",
    "그런데 인터넷은 네트워크로 연결되어 있는데, 수많은 컴퓨터 중에 특정 컴퓨터하고만 연결되고 싶다고 하면 IP address ex) 123.123.11.51 연결함. \n",
    "그러나 IP address를 특정한 주소 www.fun-coding.org 를 통해 IP adress에 접근함. \n",
    "\n",
    "서로 통신할 때 html언어로 구성된 파일을 달라고 하는 것이고, 요청을 받으면 html 파일을 전달해주어 웹브라우저 안에서 이쁜 화면으로 보내는 것. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://www.hankookilbo.com/News/Read/202001221751364738?did=NS&dtype=2&dtypecode=25835&prnewsid=')\n",
    "\n",
    "# res.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. 웹페이지 파싱하기\n",
    "\n",
    "파싱(parsing): 문자열의 의미를 분석하는 것 \n",
    "<html><body> 등의 의미를 대신 분석해주는 것이라고 생각해주면 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res의 내용을 분석하는 것이므로 res.content라 써야함.\n",
    "# parser가 여러가지 있는데, 그 중에서 html.parser을 쓰겠다!\n",
    "\n",
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. 필요한 데이터 추출하기\n",
    "\n",
    "- soup.find() 함수로 원하는 부분을 지정하면 됨\n",
    "- 변수.get_text() 함수로 추출한 부분을 가져올 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = soup.find('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. 추출한 데이터 활용하기\n",
    "\n",
    "여기서부터는 프로그래밍의 영역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "성전환 변희수 하사, 전역 판정에 “포기 않겠다…기회 달라”\n"
     ]
    }
   ],
   "source": [
    "print(mydata.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTML 이해를 바탕으로 크롤링하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"title\">[1]크롤링이란?</h1>\n",
      "[1]크롤링이란?\n",
      "[1]크롤링이란?\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html = \"\"\"\"\n",
    "<html> \n",
    "    <body> \n",
    "        <h1 id='title'>[1]크롤링이란?</h1>\n",
    "        <p class='cssstyle'>웹페이지에서 필요한 데이터를 추출하는 것</p>\n",
    "        <p id='body' align='center'>파이썬을 중심으로 다양한 웹크롤링 기술 발달</p>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "data = soup.find('h1')\n",
    "print(data)\n",
    "print(data.string)\n",
    "print(data.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSS selector 사용해서 크롤링하기 \n",
    "\n",
    "- select() 안에 태그 또는 Css class 이름등을 넣어주면 됨\n",
    "- 결과값은 리스트 형태로 반환됨\n",
    "- CSS Selector 상세 문법:  https://saucelabs.com/resources/articles/selenium-tips-css-selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h3 class=\"tit_view\" data-translation=\"\">잔금대출에도 DTI 규제 적용 검토</h3>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('http://v.media.daum.net/v/20170615203441266')\n",
    "soup = BeautifulSoup(res.content,'html.parser')\n",
    "items = soup.select('h3.tit_view')\n",
    "items\n",
    "\n",
    "# select로 가져오면 무조건 리스트로 가져온다! \n",
    "# select의 경우, 태그 안의 클라스 이름을 .을 찍어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h3 class=\"tit_view\" data-translation=\"\">잔금대출에도 DTI 규제 적용 검토</h3>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = soup.find('h3', 'tit_view')\n",
    "data1\n",
    "\n",
    "# find의 경우 쉼표로 표시해줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- class가 2개 이상일 때, .으로 연결해주면 됨\n",
    "\n",
    "Ex) data = soup.select(\"div.ah_roll_area.PM_CL_realtimeKeyword_rolling\")\n",
    "\n",
    "- 어떻게 적어야 할 지 잘 모르겠다면? \n",
    "\n",
    "꿀팁: copy - css selector - cmd+v에서 추측 가능함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 하위 태그 선택 \n",
    "\n",
    "1) items = soup.select('html head title')\n",
    "이렇게도 하위 태그를 선택해서 적을 수 있음.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "잔금대출에도 DTI 규제 적용 검토 | Daum 뉴스\n"
     ]
    }
   ],
   "source": [
    "items = soup.select('html head title')\n",
    "for item in items: \n",
    "    print(item.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 중간에 있는 모든 태그를 쓰지 않고 처음과 끝만 써도 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "잔금대출에도 DTI 규제 적용 검토 | Daum 뉴스\n"
     ]
    }
   ],
   "source": [
    "items = soup.select('html title')\n",
    "for item in items: \n",
    "    print(item.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) 바로 밑에 있는 tag를 가져와서 보는 방법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "잔금대출에도 DTI 규제 적용 검토 | Daum 뉴스\n"
     ]
    }
   ],
   "source": [
    "items = soup.select('head > title')\n",
    "for item in items: \n",
    "    print(item.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. CSS class 이름으로 검색하기\n",
    "\n",
    "class를 의미하려면 .을 앞에 적어주면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "잔금대출에도 DTI 규제 적용 검토\n"
     ]
    }
   ],
   "source": [
    "items = soup.select('.tit_view')\n",
    "for item in items: \n",
    "    print(item.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. CSS ID 이름으로 검색하기 \n",
    "\n",
    "id는 #을 앞에다가 적어주면 됨. \n",
    "여기다가 html div#kakaoIndex 이렇게 복합으로 사용해도 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "본문 바로가기\n",
      "메뉴 바로가기\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items = soup.select('div#kakaoIndex')\n",
    "for item in items: \n",
    "    print(item.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### urllib 라이브러리를 활용한 크롤링 코드\n",
    "\n",
    "- 인터넷의 다양한 예제들이 urllib 또는 urllib2를 사용한 예제들이 많음\n",
    "- 인코딩 문제가 있는 사이트의 경우 urllib 라이브러리를 사용하면 해결되기도 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = urlopen('')\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 나머지는 같음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
